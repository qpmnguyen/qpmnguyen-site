[
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "Quang Nguyen",
    "section": "",
    "text": "Here’s a snapshot of what I have been doing in the past years! You can find a complete version of my CV here. The complete collection of my publications can be found on my Google Scholar page, and my work history can be found on my LinkedIn."
  },
  {
    "objectID": "background.html#current-roles",
    "href": "background.html#current-roles",
    "title": "Quang Nguyen",
    "section": "Current roles",
    "text": "Current roles\nRegeneron Pharmaceuticals | 2022 - current\nPrincipal Biostatistician\nDELPHI Group at Carnegie Mellon University | 2022 - current\nVolunteer R Developer"
  },
  {
    "objectID": "background.html#education",
    "href": "background.html#education",
    "title": "Quang Nguyen",
    "section": "Education",
    "text": "Education\nBates College | 2017\nBachelor of Science in Biochemistry and Mathematics\nDartmouth College | 2022\nDoctor of Philosophy in Quantitative Biomedical Sciences"
  },
  {
    "objectID": "background.html#skills",
    "href": "background.html#skills",
    "title": "Quang Nguyen",
    "section": "Skills",
    "text": "Skills\n\nProficient with R for statistical analysis and data science: Using tidyverse for data wrangling and visualization, targets for workflow generation, tidymodels for predictive modelling. Experienced in developing R packages and Shiny dashboards.\nAdditional languages:\n\nFamiliar with leveraging Julia/Rcpp/Rust for performance intensive tasks.\n\nFamiliar with Python for deep learning (tensorflow).\n\nFamiliar with Stan for Bayesian analysis (via cmdstanr).\n\nAdditional skills include:\n\nShell/Bash scripting for running command-line tools and utilities.\nGit/GitHub for code versioning and continuous integration.\n\nSQL (SQLite and MySQL) for creating and accessing data that cannot fit in memory.\n\nSlurm/Torque High Performance Computing for analyses that require extra memory and computing time.\n\nsnakemake/nextflow for data-intensive workflow management."
  },
  {
    "objectID": "background.html#research-highlights",
    "href": "background.html#research-highlights",
    "title": "Quang Nguyen",
    "section": "Research highlights",
    "text": "Research highlights\nMy doctoral dissertation was focused on leveraging external biological knowledge and multi-omics data sets to understand gut microbiomes surveyed in population health studies. I am currently working on exploratory analyses supporting early phase clinical trials. A complete list of my publications (incl. co-author publications) can be found on my Google Scholar page.\nQuang P Nguyen, H. Robert Frost, Anne G. Hoen et al. Associations between the gut microbiome and the metabolome in early life. BMC Microbiology. Aug 2021. [Source Code][Manuscript]\nQuang P Nguyen, Anne G. Hoen, H. Robert Frost. CBEA: Competitive isometric log-ratio for taxonomic enrichment analysis. PLoS Computational Biology. Jun 2022. [Code] [Package] [Manuscript]\nQuang P Nguyen, Anne G. Hoen, H. Robert Frost. Evaluating trait-based databases for taxonomic enrichment analysis. Pre-print on bioRxiv. June 2022. [Code] [Pre-print]"
  },
  {
    "objectID": "posts/20210110-election_elo/index.html",
    "href": "posts/20210110-election_elo/index.html",
    "title": "Election prediction using Elo",
    "section": "",
    "text": "This post is a re-adaptation and R-implementation of my previous post on a fun modeling project using Elo to predict the latest 2020 presidential elections. The previous implementation was done in Julia and can be found in this interactive Pluto.jl document. You can find the source code for the Julia implementation here. For this exercise, first let’s load our libraries:\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(jsonlite)\nlibrary(glue)\nlibrary(ggthemes)\nlibrary(showtext)\nfont_add_google(\"Roboto Slab\")\nshowtext_auto()"
  },
  {
    "objectID": "posts/20210110-election_elo/index.html#what-is-an-elo-model",
    "href": "posts/20210110-election_elo/index.html#what-is-an-elo-model",
    "title": "Election prediction using Elo",
    "section": "What is an Elo model?",
    "text": "What is an Elo model?\nThe elo rating system is a method developed by Arpad Elo in order to rank players by ability in a zero sum game. Elo has been used in various contexts, most famously the way to rank chess players through the UCSF and FIDE systems. Elo has been used in modern contexts as well, from Tinder to World of Warcraft. A really nice attribute about Elo is that a difference in rankings can be translated to a probability. Let’s assume a focal player \\(A\\) with rank \\(R_A\\) is facing up against opponent \\(B\\) with rank \\(R_B\\). The expected “score” \\(E_A\\) of player \\(A\\) in the match can be defined as:\n\\[E_A = \\frac{1}{1 + 10^{(R_B - R_A)/400}} = \\frac{1}{1 + 10^{-(R_A - R_B)/400}}\\]\nRecall that the standard logistic function is \\(\\frac{1}{1 + e^{-x}}\\). You can see that there are tons of similarities between the standard logistic function and the function for the expectation of player \\(A\\)’s score, with the exception being the exponential constant \\(e\\). This is because while elo probabilities are log-odds, they are not natural log based but rather based on base 10. There are easy conversions between the base 10 and base \\(e\\), which allows sports statisticians to perform logistic regression on elo rankings. However, that is a bit beyond the focus of this project.\nThe elo model updates the ranking of the person after each match. For our focal player \\(A\\), the rankings are updated as:\n\n\\[R_A* = R_A + K \\cdot (outcome - E_A)\\] where \\(K\\) is the \\(K\\)-factor controlling the relative gains and losses of elo points. The variable outcome here is a binary variable (1 if player \\(A\\) wins). Think of this similar to how you would use a binary cross-entropy loss to train a model that outputs an odds (such as a logistic regression) instead of true binary values. In this case, ranking updates are based on the difference between the true outcome and the expected outcome \\(E_A\\). Let’s define a couple of functions reflecting the ideas above:\n\n\nCode\n# Calculate win probabilities from two rankings  \nwin_prob <- function(r1, r2){\n  x1 <- 10^((r2 - r1)/400)\n  p1 <- 1/(1 + x1)\n  p2 <- 1 - p1\n  return(list(p1 = p1 , p2 = p2))\n}\n\n# Back transform from probability to difference in rankings \nback_transformation <- function(prob){\n  diff <- 400 * log((1/prob) - 1,base = 10)\n  # we want difference between the focal person, which means we have to invert this\n  return(diff * -1)\n}\n\n\nThe first function, win_prob calculates the win probability given two rankings. It outputs a list with p1 being the win probability of the focal player 1. The second function, back_transformation, takes in the probability and returns a difference in rankings.\nTo test and see whether our model is correct, let’s compare to a reference. According to this website, a win probability of 0.76 for focal player 1 would be a result of 200 elo difference. Let’s try it out!\n\n\nCode\nglue(\"Win probability of 76% converts to an elo difference of {diff}\", diff = back_transformation(0.76))\n\n\nWin probability of 76% converts to an elo difference of 200.240940227674\n\n\nSo our elo formulation so far is correct."
  },
  {
    "objectID": "posts/20210110-election_elo/index.html#the-us-presidential-election-as-a-pseudo-sports-contest",
    "href": "posts/20210110-election_elo/index.html#the-us-presidential-election-as-a-pseudo-sports-contest",
    "title": "Election prediction using Elo",
    "section": "The US presidential election as a pseudo-sports contest",
    "text": "The US presidential election as a pseudo-sports contest\nThe US presidential election is a surprisingly appropriate context to use elo. In this election, two candidates compete for the “title” of the President of the United States. Polling data is collected regularly, which can act as “matches” where the two candidates can test their “mettle” before the final election day. Furthermore, the structure of the election, a winner-take-all Electoral College system, makes it even closer to being a sports match.\nIn this instance, elo system is applied to model the probability of winning an election. Weekly average polling results are used as proxy for “games”. After each weekly poll, the candidate gains or lose elo based on whether they win or lose (polling average > 50%) and how much did they win (or lose) by. Some major caveats and assumptions:\n\nFirst, this model is for fun, and does not consider variables such as demographics, economics, recent news, etc., which are considered part of the fundamentals portion of any reliable forecasting model. Since the model ingests polling averages from FiveThirtyEight, it does account for quality of polling and other associated factors as a proxy.\nSecond, the model does not account for random effects occuring at the state level or across time, as each unit of polling is considered to be independent.\nThird, the model rewards consistent polling, and rewards them with higher probability of winning. I would say this is a reasonable assumption as performance in the polls with high margins often indicate a victory in that jurisdcition. However, as many consumers of election modelling know, factors such as close to election day scandals or turnout would affect the final results and violate this assumption."
  },
  {
    "objectID": "posts/20210110-election_elo/index.html#methods",
    "href": "posts/20210110-election_elo/index.html#methods",
    "title": "Election prediction using Elo",
    "section": "Methods",
    "text": "Methods\n\n“Training” data\nLet’s start our modeling exercise by reading in basic polling data from FiveThirtyEight. Here, we simplify things a little bit. First, we do not consider the split votes of the states of Nebraska and Maine, and second, we only focus on the adjusted percentage scores, which usually accounts for polling reliability.\n\n\nCode\ndata <- read.csv(file = \"data/presidential_poll_averages_2020.csv\")\nproc_data <- data %>% as_tibble() %>% mutate(modeldate = as_date(strptime(modeldate, format = \"%m/%d/%Y\"))) %>% # reformat names\n  mutate(candidate_name = case_when(\n    candidate_name == \"Convention Bounce for Joseph R. Biden Jr.\" ~ \"Joseph R. Biden Jr.\",\n    candidate_name == \"Convention Bounce for Donald Trump\" ~ \"Donald Trump\", \n    TRUE ~ candidate_name\n  )) %>% mutate(week = week(modeldate)) %>% filter(!state %in% c(\"NE-1\", \"NE-2\", \"ME-1\", \"ME-2\", \"National\")) %>% # for simplicity\n  group_by(state, week, candidate_name) %>% # get week \n  summarise(poll_avg = mean(pct_trend_adjusted)) # average per week \nrmarkdown::paged_table(proc_data)\n\n\n\n\n  \n\n\n\n\n\nPrior information for initial elo seeding\nJust having an elo model is not enough! We want to seed our “players” with some elo first. The easiest way is to seed both candidates with an elo of 1000, but that doesn’t account for each candidate’s advantage at each state due to their political affiliation. Here, we decided to take a simple solution and take the election results from the 2016 election. Based on the margin of victory, we will use the back_transformation function to calculate a difference in elo and use that as a seed. For example, if Donald Trump won state X with 76% of the votes, then he would start with 1200 elo compared to Joe Biden’s 1000 elo. As such, we are able to incorporate some prior information. We want polling to be good enough to overcome this inherent advantage.\n\n\nCode\n# Function to convert abbreviation to state name\nget_stname <- function(state_abb){\n  if (state_abb == \"DC\"){\n    name <- \"District of Columbia\"\n  } else {\n    name <- state.name[grep(state_abb, state.abb)]\n  }\n  if (rlang::is_empty(name)){\n    return(NA_character_)\n  } else {\n    return(name)\n  }\n}\n# Prior information \nprior <- read.csv(file = \"data/returns-2016.csv\") %>% as_tibble()\n\n# convert state abbreviation to name, renormalize to two party candidates, then back transform prob to difference in elo\nprior <- prior %>% mutate(state = map_chr(state_abbreviation, ~get_stname(.x))) %>% \n  mutate(Clinton = Clinton/(Clinton + Trump), Trump = Trump/(Clinton + Trump)) %>% \n  select(-c(state_abbreviation, Other)) %>% \n  mutate(elo_diff = back_transformation(Clinton))\nrmarkdown::paged_table(prior)\n\n\n\n\n  \n\n\n\n\n\nIncorporating margin of victory\nA lot of elo models also incorporate something called the margin of victory. Margin of victory (mov) scales elo gains and losses based on how well the player performed. A knock-out match from an underdog would result in a massive gain in elo, compared to if that match was a close one. We try to incorporate margin of victory into our elo update calculation by modifying the K-factor with a linear adjustment \\[K_{eff} = K * mov\\] where \\[mov\\] is the margin of victory. This means that the “winning” candidate will gain points as a proportion of K that is equal to their margin of polling victory. As such, we increase values of K significantly to compensate for the low gains in elo when margins can be 1 percent. This also prevents polling results from driving elo scores to crazy levels.\nTaking that into account, here’s out update function.\n\n\nCode\n# Update elo using those rankings  \n\nupdate_elo <- function(r1, r2, outcome, mov, K=30){\n  probs <- win_prob(r1, r2)\n  p1 <- probs$p1\n  p2 <- probs$p2\n  K = K * mov # K-gains as a proportion of margin of victory  \n  r1_new <- round(r1 + K*(outcome - p1), digits = 0)\n  r2_new <- round(r2 + K*((1 - outcome) - p2), digits = 0)\n  return(list(r1 = r1_new, r2 = r2_new))\n}\n\n\n\n\nGetting elo sequence\nThe general gist of the model would be getting the elo sequence for each state based on weekly average polling, with prior information from the 2016 election. We’re going to choose the Democratic candidate as the focal candidate, with rankings equals r1. With that in mind, a wrapper function that performs this for each state.\n\n\nCode\nget_sequence <- function(proc_data, prior, st, init=1000, K = 50){\n  df <- proc_data %>% filter(state == st) # filter by state\n  weeks <- unique(df$week) # get all unique weeks \n  elo <- vector(mode = \"list\", length = length(weeks) + 1) # initialize elo vector\n  prior_diff <- prior %>% filter(state == st) %>% pull(elo_diff)\n  # if Trump won state then prior_diff < 0 since Democrats are the focal candidate\n  if (prior_diff < 0){\n    elo[[1]] <- tibble(\n      DT_elo = init + abs(prior_diff),\n      JB_elo = init\n    )\n  } else if (prior_diff > 0){\n    elo[[1]] <- tibble(\n      DT_elo = init,\n      JB_elo = init + abs(prior_diff)\n    )\n  } else {\n    elo[[1]] <-  tibble(\n      DT_elo = init,\n      JB_elo = init\n    )\n  }\n  # iterate \n  for (i in 2:length(elo)){\n    elos <- update_elo(r1 = elo[[i-1]]$JB_elo, \n                       r2 = elo[[i-1]]$DT_elo,\n                       outcome = df$win[i-1], \n                       mov = df$mov[i-1], K = K)\n    elo[[i]] <- tibble(\n      DT_elo = elos$r2,\n      JB_elo = elos$r1,\n    )\n  }\n  seq <- do.call(rbind, elo)\n  weeks <- c(min(weeks)-1, weeks)\n  seq <- seq %>% mutate(state = rep(st, length(elo)), week = weeks)\n  seq <- seq %>% select(state, week, DT_elo, JB_elo) %>% \n    pivot_longer(c(DT_elo, JB_elo), names_to = \"candidate\", values_to = \"elo\") %>% \n    mutate(candidate = recode(candidate, DT_elo = \"DT\", JB_elo = \"JB\")) %>% \n    pivot_wider(names_from = candidate, values_from = elo)\n  return(seq)\n}\n\n\nThe implementation for this is a bit weird, since we have a time 0 where the candidates are seeded. But before we apply this to our data, first let’s process further the basic polling and assigning a win or loss based upon the polling percentages to generate our \\(outcome\\). If the poll is tied, a random draw from the binomial distribution is performed.\n\n\nCode\nproc_data <- proc_data %>% \n  mutate(candidate_name = case_when(candidate_name == \"Donald Trump\" ~ \"DT\", TRUE ~ \"JB\")) %>% \n  pivot_wider(names_from = \"candidate_name\", values_from = \"poll_avg\") %>% \n  mutate(win = case_when(JB > DT ~ as.integer(1), \n                         JB < DT ~ as.integer(0), \n                         JB == DT ~ rbinom(1,1,0.5))) %>% \n  mutate(mov = abs(DT - JB)/100)\nrmarkdown::paged_table(proc_data)\n\n\n\n\n  \n\n\n\nApplying our elo sequences model for all states.\n\n\nCode\nget_elo_all_states <- function(proc_data, prior, K=50, init=1000){\n  states <- unique(proc_data$state)\n  data <- vector(mode = \"list\", length = length(states))\n  for (i in seq_len(length(states))){\n    data[[i]] <- get_sequence(proc_data, prior, states[i], K = K, init = init)\n  }\n  return(do.call(dplyr::bind_rows, data))\n}\n\nelo_sequences <- get_elo_all_states(proc_data, prior)\nelo_sequences <- elo_sequences %>% rowwise() %>% mutate(prob = win_prob(r1 = JB, r2 = DT)$p1)\nrmarkdown::paged_table(elo_sequences)\n\n\n\n\n  \n\n\n\n\n\nFinal probabilities using Monte Carlo simulations\nTo determine the final probability of winning the election, we take our state-level probabilities and simulate binomial draws with our probabilities. This is similar to how FiveThirtyEight performs their final modelling as well. The final win chance is therefore the the number of simulations where a focal candidate (in this case Biden) gains 270 or more Electoral Votes.\n\nProcessing electoral votes data\n\n\nCode\nelectoral_votes <- jsonlite::fromJSON(\"data/electoral_votes.json\")\nelectoral_votes <- do.call(rbind, electoral_votes) \nelectoral_votes <- electoral_votes %>% as.data.frame() %>% rename(\"votes\" = \"V1\") %>% \n  rownames_to_column(var = \"state\") %>% \n  mutate(state = map_chr(state, get_stname)) %>%\n  na.omit() %>% as_tibble()\n\n\n\nLet’s create another wrapper function that wraps all of this together in one go:\n\n\nCode\nelo_predict <- function(proc_data, prior, e_votes, K = 50, init = 1000, n_iter = 1000){\n  # first, let's get all elo sequences \n  elo_sequences <- get_elo_all_states(proc_dat = proc_data, prior = prior, K = K, init = init)\n  # then, let's get probabilities from them \n  elo_sequences <- get_elo_all_states(proc_data, prior)\n  elo_sequences <- elo_sequences %>% rowwise() %>% mutate(prob = win_prob(r1 = JB, r2 = DT)$p1)\n  # then, let's retrieve the probabilities by state at the last week \n  # (which.max allows for selecting the most recent week)\n  suppressMessages(probs <- elo_sequences %>% group_by(state) %>% summarise(prob = prob[which.max(week)]))\n  # merge electoral votes with probability data \n  probs <- left_join(probs, e_votes, by = \"state\")\n  # predict and multiply with votes \n  suppressMessages(preds <- map_dfc(seq_len(n_iter), ~{\n    map_int(probs$prob, ~rbinom(1,1,.x)) * probs$votes\n  }))\n  # rename \n  names(preds) <- paste0(\"sim_\",seq_len(n_iter))\n  # retrieve state names and swtich columns\n  preds <- preds %>% mutate(state = probs$state) %>% select(state, everything())\n  return(list(preds = preds, elo_seq = elo_sequences))\n}"
  },
  {
    "objectID": "posts/20210110-election_elo/index.html#results",
    "href": "posts/20210110-election_elo/index.html#results",
    "title": "Election prediction using Elo",
    "section": "Results",
    "text": "Results\nNow that we have everything set up, let’s perform some predictions!\nWe use the default \\(K\\)-factor of 50 and the initial elo seeding of 1000 with 1000 simulations. Since elo operates mostly from the differences in elo rankings, the initial elo seeding is not as important.\n\n\nCode\npred <- elo_predict(proc_data = proc_data, prior = prior, e_votes = electoral_votes, K = 50, init = 1000)\nrmarkdown::paged_table(pred$preds)\n\n\n\n\n  \n\n\n\nWith our predictions, we can tally up the number of times the focal candidate JB wins over 270 electoral votes, which can get us an estimate of the overall probability. We can also do some bootstrap resamplings of the already generated 1000 simulations to get an bootstrapped interval.\n\n\nCode\nvote_counts <- colSums(pred$preds[,-1]) %>% unname()\nprobability <- round((length(which(vote_counts >= 270))/1000)*100, digits = 2)\n\nboot <- map_dbl(seq_len(1e4), ~{\n  counts <- sample(vote_counts, size = 1000, replace = T) \n  round((sum(counts >= 270)/1000)*100, digits = 2)\n})\n\nglue(\"Win probability of Biden is {prob}% ({lower}% - {upper}%)\", \n     prob = probability, lower = quantile(boot, 0.025), upper = quantile(boot, 0.975))\n\n\nWin probability of Biden is 80% (77.5% - 82.4%)\n\n\nThis is a pretty reasonable assumption. Let’s explore the uncertainty around our probability model a bit further. Let’s first visualize the distribution of votes across all simulations\n\n\nCode\nqtile <- quantile(vote_counts, probs = c(0.025, 0.975))\n\nplt <- qplot(x = vote_counts, geom = \"histogram\", fill = I(\"#5657B8\"), alpha = I(0.8), binwidth = 10) + \n  theme_bw() + \n  labs(x = \"Electoral Vote Counts\", y = \"Frequency\", title = \"Distribution of total electoral vote counts\",\n       subtitle = \"1000 simulations\") + \n  geom_vline(xintercept = qtile[1], col = \"red\", size = 1.5) + \n  geom_vline(xintercept = qtile[2], col = \"red\", size = 1.5) + \n  annotate(geom = \"curve\", x = 175, y = 75, xend = qtile[1], yend = 50, \n           curvature = 0.3, arrow = arrow(length = unit(2, \"mm\"))) +\n  annotate(geom = \"text\", x = 175, y = 83, label = glue(\"0.025 quantile \\n {val}\", val = qtile[1])) +\n  annotate(geom = \"curve\", x = 420, y = 75, xend = qtile[2], yend = 50, \n           curvature = 0.2, arrow = arrow(length = unit(2, \"mm\"))) +\n  annotate(geom = \"text\", x = 420, y = 83, label = glue(\"0.075 quantile \\n {val}\", val = qtile[2])) +\n  theme(text = element_text(famil = \"Roboto Slab\", size = 13))\nprint(plt)\n\n\n\n\n\nWe can visualize the trajectory of elo for each state. Let’s take three states: California, Pennsylvania and Arkansas. These three states represents three types of states you would usually see: solidly blue, swing, and solidly red.\n\n\nCode\nseq <- pred$elo_seq\nseq <- seq %>% filter(state %in% c(\"California\", \"Pennsylvania\", \"Montana\"))\nseq <- seq %>% pivot_longer(c(DT, JB))\nggplot(seq, aes(x = week, y = value, col = name)) + geom_line() + geom_point() + \n  facet_grid(state~., scales = \"fixed\") + \n  scale_color_brewer(palette = \"Set1\", labels = c(\"Donald Trump\", \"Joe Biden\")) + theme_bw() + \n  labs(y = \"Elo sequence\", x = \"Polling week\", col = \"Candidate\", \n       title = \"Weekly elo rankings for each candidate\") + theme(text = element_text(family = \"Roboto Slab\"))\n\n\n\n\n\nThe elo rankings here makes sense. For states like California and Montana whose political affiliation is well known, the elo difference is larger, and continue to increase throughout the week due to good polling. The margin of victory also scales accordingly, where the rate of elo gain for Biden in California and Montana are really high. Conversely, Pennsylvania, while right now heading in Biden’s direction, do not change as much. The margin of victory was probably too small to generate a wide gap as that of the other two states, indicating a close race. However, it seems that the polling is good enough for Joe Biden to overcome the results of the 2016 election.\nFinally, let’s generate a fun visual similar to FiveThirtyEight’s bubble plots!\n\n\nCode\nset.seed(1234)\ncounts <- tibble(counts = sample(vote_counts, size = 100, replace = T)) %>% arrange(desc = TRUE) %>% \n  mutate(cat = rep(\"simulation\", 100)) %>% mutate(counts = counts - (538 - counts)) %>% \n  mutate(win = case_when(\n    counts > 0 ~ \"Win\",\n    counts == 0 ~ \"Tie\",\n    counts < 0 ~ \"Loss\"\n  ))\nggplot(counts, aes(x = cat, y = counts, col = win)) + geom_jitter(size = 5, alpha = 0.8) + theme_minimal() + \n  coord_flip() + geom_hline(yintercept = 0) + \n  theme(axis.line.y = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank(), \n        axis.text.y = element_blank(), panel.grid.major = element_blank(), \n        text = element_text(family = \"Roboto Slab\"),\n        panel.grid.minor = element_blank()) +\n  scale_y_continuous(breaks=c(-300,-150,0,150,300)) + scale_color_brewer(palette = \"Set1\") +\n  labs(y = \"Electoral vote differences\", col = \"Result for Biden\") +\n  annotate(\"text\", y = 150, x = 1.5, \n           label = glue(\"Chance of winning \\n {val} in 100\", val = sum(counts$counts >= 0)))"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Quang Nguyen",
    "section": "",
    "text": "CBEA: This R package implements the CBEA (Competitive balances for taxonomic enrichment analysis) approach. More details can be found in the corresponding manuscript. Source code can be found on GitHub and can be installed via Bioconductor.[site]\n\nnprbooksR: This is an R data package containing scraped information from NPR’s book concierge (now known as Books We love). Built using RSelenium. [Source]\n\nDIFUSE Eddy Covariance Exploration: A Shiny app for BIOL 16 course at Dartmouth developed as part of the DIFUSE program (funded by NSF) for interactive data visualization and basic model building. [Source]\n\nTidyTuesday: Personal repository of figures and tables participating in #TidyTuesday events. [Source]"
  },
  {
    "objectID": "projects.html#open-source-contributions",
    "href": "projects.html#open-source-contributions",
    "title": "Quang Nguyen",
    "section": "Open-source contributions",
    "text": "Open-source contributions\n\nepiprocess: This is an R package developed by the DELPHI group at Carnegie Mellon University to provide tools for basic signal processing in epidemiological forecasting. [Source]\n\nSBICgraph: Structural Bayesian Information Criterion (SBIC) for model selection in network models with a two-step algorithm to generate candidate model pool. Manuscript at Zhou et al. 2021. [Source]"
  },
  {
    "objectID": "projects.html#non-research-writings",
    "href": "projects.html#non-research-writings",
    "title": "Quang Nguyen",
    "section": "Non-research writings",
    "text": "Non-research writings\nI wrote some articles as part of the The COVID Tracking Project at The Atlantic.\n\nFederal COVID Data 101: Working with Death Numbers\nWe Don’t Know How Many People Have Recovered From COVID-19. (Cross-published at The Atlantic)\nAs North Dakota’s Deaths Metrics Diverge, We’re Switching to a Less Backlogged Measure of Fatalities\nThe State of State Antigen Test Reportings\nConfirmed and Probable COVID-19 Deaths, Counted Two Ways\nWhat Is a Probable Case of COVID-19?\nPosition Statement on Antibody Data Reporting"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I’m a data scientist and R/Julia developer in the biomedical sciences. I am currently a Biostatistician at Regeneron Pharmaceuticals in early clinical research. Previously, I received my Ph.D. at Dartmouth College working on statistical methods for integrating prior biological knowledge in microbiome data analysis tasks."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "A naive statistician’s blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nElection prediction using Elo\n\n\n\n\n\n\n\nFun\n\n\nElo\n\n\nanalysis\n\n\n\n\nA fun project to predict the 2020 presidential election using weekly average polling data and Elo models\n\n\n\n\n\n\nJan 10, 2021\n\n\nQuang Nguyen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "analysis_philosophy.html",
    "href": "analysis_philosophy.html",
    "title": "Quang Nguyen",
    "section": "",
    "text": "This is a work in progress\nVersion: 1.0\nMy analysis philosophy here refers to a set of principles around data analysis that I’ve come to rely on whenever I approach a data problem. The obvious caveat here is that every project is different and that there is no standard way to go about it. The goal is to document a data science “world-view” that represents my values as a data scientist.\nThis document is therefore definitely influenced by my background and current employment. My training is interdisciplinary across the fields of biostatistics, bioinformatics, and epidemiology, which means that I carry a little bit of the DNA of each field forward to my work. As such, while I am a fan of computational/algorithmic driven approaches (e.g., machine learning), I give a lot of weight to the more “traditional” aspects of data analysis such as study design and inferential statistics. This is reinforced further as I am currently working as a biostatistician in a biopharmaceutical context.\nThis document is not a fixed-in-place kind of philosophy. I intend to update it with new things as I learn."
  },
  {
    "objectID": "analysis_philosophy.html#how-i-approach-data-analysis",
    "href": "analysis_philosophy.html#how-i-approach-data-analysis",
    "title": "Quang Nguyen",
    "section": "How I approach data analysis",
    "text": "How I approach data analysis\n\nIt’s all about the data generating process\nLet’s start with a definition\n\nThe data generating process (DGP)1 refers to the underlying process that give rise to a data point, which includes aspects such as: whose data is being collected, how is the collection being done, when is the data collection performed, etc.1 More formal DGP definitions would include the function specifying the relationship between dependent and independent variables such as the typical linear regression function Y = \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon. Here, we refer to the conceptual idea of tracing the provenance of a data point.\n\nIt is a truth universally acknowledged that the underlying DGP for a given real world data set is never going to be known. However, regardless of whether or not it can be observed, it contains analytical consequences and affect which the assumptions we can make in our models. Therefore, thinking thoroughly about how a data set might arise is crucial to making good decisions around not just what models to choose but also how to pre-process and wrangle data. For example, a commonly-known-yet-unknown fact is that when you remove subjects with missing data points (i.e., the complete case analysis), you’re making the assumption that the data is missing completely at random (MCAR)2. Making this assumption for convenience of the mechanics of model fitting can create biased and potentially misleading results. Obviously there is no way to properly account for everything, but mapping out how a data point moves from the metaphorical truth to what is presented on the spreadsheet can be the difference maker.2 Little, R. J. A., D. B. Rubin, and S. Z. Zangeneh. 2017. \"Conditions for Ignoring the Missing-Data Mechanism in Likelihood Inferences for Parameter Subsets.\" Journal of the American Statistical Association 112 (517): 314–20.\nIt is also a mistake to think that prediction problems are free of issues relating to DGP. A famous instance would be the ethically dubious paper that seeks to predict criminality based on facial recognition. Moral issues aside, the authors trained their convolutional neural network based on a data set where all of their “positive” instances are all pictures taken from mug shots while their “controls” or “negative” instances are all “normal” photos. Thinking about the DGP can avoid these confounding problems and requires the analyst to meet the data face-to-face (pun not intended).\n\nWhat does thinking through a data problem even mean?\n\n\nIn a reductive sense, trying to rationalize through the data generating process of observational (and experimental) data sets is essentially the entire field of epidemiology. I am definitely influenced by works such as Epidemiological Methods3 by Koepsell and Weiss as well as the more recently published targeted trial framework4 by Hernan and Robbins.3 Koepsell, Thomas D., and Noel S. Weiss. Epidemiologic methods: studying the occurrence of illness. Oxford University Press, USA, 2014.4 Hernán, Miguel A., and James M. Robins. “Using big data to emulate a target trial when a randomized trial is not available.” American journal of epidemiology 183.8 (2016): 758-764.\nIt’s a hard question to answer, but I usually rely on trying to identify some core component of each data point, namely:\n\nI generally want to know what is the most basic unit of observation. I generally think of this in terms of inclusion/exclusion criteria, which would the population that will be captured in this data set. Comparing it to what would be expected is important to figure out whether results are going to apply to the question of interest. I’ll also look at whether these observations are independent, or clustered in some form (e.g., geography).\nThen, I want to figure out what exactly is being measured at each data point and the operational characteristics of those data. Sometimes, we cannot measure a value directly and have to rely on proxies. This is especially important for the quantity I want to model. If there is missing data, I will attempt to figure out a way to explain the mechanism of missingness. Another important component of measurements will also be the circumstances taking the measurement as well. For example, taking a survey in front of an employee is very different than taking a survey remotely.\nI also want to identify what most likely influences the quantity of interest coming from the unit of observation. In other words, what are potential predictors (covariates) that can explain the measurement of interest? I also want to stratify these variables in terms of both those with a potential causal mechanism as well as those who are spurious (confounders).\n\nMost of the time, these questions are very self-evident, which is great, but most of the time it doesn’t., so it’s never a waste of time to think about these questions in great detail (and throughout the entire project lifetime!)\n\n\n\nPre-specification is king\n\n\nPrincipled scaling of model complexity\n\n\nUncertainty is more interesting than the mean"
  },
  {
    "objectID": "analysis_philosophy.html#additional-important-facets-of-data-analysis",
    "href": "analysis_philosophy.html#additional-important-facets-of-data-analysis",
    "title": "Quang Nguyen",
    "section": "Additional important facets of data analysis",
    "text": "Additional important facets of data analysis\n\nDocumentation underlies reproducibility"
  }
]